---
title: "36-315 Homework 9, Fall 2019"
author: "Eu Jing Chua"
date: "Due Nov 13, 2019 (11pm) on Canvas"
output: 
  html_document:
    toc:  true
    toc_float:  true
    code_folding:  hide
---

##  Homework 9

```{r}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

```{r}
library(tidyverse)
library(viridis)

eujingc_315_theme <-  theme_bw() +
  theme(axis.text = element_text(size = 12),
        plot.title = element_text(size = 20, face = "bold", hjust = 0),
        plot.subtitle = element_text(size = 14, face = "italic", hjust = 0),
        text = element_text(size = 14, face = "bold", color = "darkslategrey"))
```

Using the tidyverse style guide.

## Problem 1

(a) I chose an alpha level of 0.0000001.

```{r}
library(igraph)
library(ggraph)

g_theme <- theme(axis.text.x = element_blank(),
          axis.title.x = element_blank(),
          axis.text.y = element_blank(),
          axis.title.y = element_blank())

red_wines <- read_csv("wineQualityReds.csv")
C <- cor(red_wines)
alpha <- 0.0000001
n <- nrow(red_wines)
Z <- 0.5 * log((1 + C) / (1 - C))
critical_value <- - qnorm(alpha / 2) / sqrt(n - 3)
A <- (abs(Z) > critical_value) * 1.0
diag(A) <- 0
g <- graph_from_adjacency_matrix(A, mode = "undirected")
ggraph(g, layout = "fr") +
    geom_edge_link(color = "grey") +
    geom_node_point() +
    geom_node_text(aes(label = name), repel = TRUE) +
    labs(title = "Dependences for Red Wines") +
    eujingc_315_theme +
    g_theme
```


(b) From the dependence graph, we can see at this alpha level that citric acid, density, sulphates, volatile acidity, alcohol and total sulfur dioxide are related to quality.

(c)
```{r}
library(SIN)

out <- sinUG(var(red_wines), n)
alpha <- 0.01
diag(out) <- 1
A <- 1.0 * (out < alpha)
g <- graph_from_adjacency_matrix(A, mode = "undirected")
ggraph(g, layout = "fr") +
    geom_edge_link(color = "grey") +
    geom_node_point() +
    geom_node_text(aes(label = name), repel = TRUE) +
    labs(title = "Conditional Independences for Red Wines") +
    eujingc_315_theme +
    g_theme
```

The variables related to quality according to the conditional independence graph are:

* Total sulfur dioxide
* Volatile acidity
* Chlorides
* Alcohol
* Sulphates




##  Problem 2

a.  The `unnest_token()` function tokenizes texts into logical chunks such as sentences and paragraphs.

b.  The column `text` contains the actual text of the tweets. From the word cloud, we can see the most common words are flight, united, as well as the names of some airlines.

```{r, warning = F, message = F}
library(tidyverse)
library(tidytext)
library(wordcloud)
data(stop_words)

airline_tweets <- read_csv("https://raw.githubusercontent.com/mateyneykov/315_code_data/master/data/Tweets.csv")

my_tweets <- dplyr::select(airline_tweets, tweet_id, text) %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100)) 
```

c.  As expected, the most common words within each airline is the airline name and "flight". However, we can also see that most airlines except Delta and Virgin America have the word "cancelled" being frequent, and United and US airways have the words "customer" and "service" being more frequent than the rest the other airlines.

```{r}
library(ggrepel)
my_tweets_airlines <- dplyr::select(airline_tweets, tweet_id, text, airline) %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  count(word, airline, sort = TRUE) %>%
  top_n(100)

ggplot(my_tweets_airlines) +
    aes(x = 1, y = 1, size = n, label = word) +
    geom_text_repel(segment.size = 0, force = 50) +
    scale_size(range = c(1, 15), guide = FALSE) +
    scale_x_continuous(breaks = NULL) +
    scale_y_continuous(breaks = NULL) +
    facet_wrap(~ airline, nrow = 2) +
    labs(x = "", y = "",
         title = "Wordcloud of Tweets by Airlines") +
    scale_color_viridis(discrete = TRUE) +
    eujingc_315_theme
```





## Problem 3

a.  
```{r}
library(ggrepel)

word_counts <- airline_tweets %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words)

word_counts %>%
    count(word, airline, sort = TRUE) %>%
    top_n(50) %>%
ggplot() +
    aes(x = 1, y = 1, size = n, label = word, color = airline) +
    geom_text_repel(segment.size = 0, force = 50) +
    scale_size(range = c(1, 15), guide = FALSE) +
    scale_x_continuous(breaks = NULL) +
    scale_y_continuous(breaks = NULL) +
    labs(x = "", y = "", color = "Airline",
         title = "Wordcloud of Airline Tweet") +
    scale_color_viridis(discrete = TRUE) +
    eujingc_315_theme
```


b.  
```{r}
word_counts %>%
    count(word, airline, user_timezone, sort = TRUE) %>%
    top_n(100) %>%
ggplot() +
    aes(x = 1, y = 1, size = n, label = word, color = user_timezone) +
    geom_text_repel(segment.size = 0, force = 50) +
    scale_size(range = c(1, 10), guide = FALSE) +
    scale_x_continuous(breaks = NULL) +
    scale_y_continuous(breaks = NULL) +
    labs(x = "", y = "", color = "Timezone",
         title = "Wordcloud of Timezones of Airline Tweets",
         subtitle = "By Airlines") +
    facet_wrap(~ airline) +
    scale_color_viridis(discrete = TRUE) +
    eujingc_315_theme
```


c.  
We can see that the frequency of tweets with the airline name for Delta and US Airways from users in the Central timezone is much less than the other airlines. Other than that, we see similar patterns in each airline's tweet word cloud with the highest frequencies being the airline name itself and "flight".

d.  
```{r}
word_counts %>%
    inner_join(get_sentiments("bing")) %>%
    count(word, sentiment, airline, sort = TRUE) %>%
    top_n(100) %>%
ggplot() +
    aes(x = 1, y = 1, size = n, label = word, color = sentiment) +
    geom_text_repel(segment.size = 0, force = 50) +
    scale_size(range = c(1, 10), guide = FALSE) +
    scale_x_continuous(breaks = NULL) +
    scale_y_continuous(breaks = NULL) +
    labs(x = "", y = "", color = "Sentiment",
         title = "Wordcloud of Sentiments of Airline Tweet",
         subtitle = "By Airlines") +
    facet_wrap(~ airline) +
    scale_color_viridis(discrete = TRUE) +
    eujingc_315_theme
```


e.  We can see from the plot that United and US Airways has the highest amount of negative sentiments compared to other airlines, mainly pertaining to delays and lost items. Southwest also seems to have the largest proportion of positive sentiments with than any other airlines.

***
***

##  Problem 4

We can see that the first topic is based more on financial words like quantities and markets, while the second topic is based more on sports-related words like cricket, captain and match.

```{r}
library(topicmodels)
articles <- read_csv("Articles.csv")

articles <- articles %>% mutate(document = rownames(articles))
articles_td <- articles %>%
    unnest_tokens(word, Article) %>%
    anti_join(stop_words) %>%
    count(document, word)

articles_dtm <- articles_td %>% cast_dtm(document, word, n)
articles_lda <- LDA(articles_dtm, k = 2, control = list(seed = 42))
articles_topics <- tidy(articles_lda, matrix = "beta")
articles_top_terms <- articles_topics %>%
    group_by(topic) %>%
    top_n(10, beta) %>%
    ungroup() %>%
    arrange(topic, -beta)

articles_top_terms %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(x = term, y = beta, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free") +
    labs(x = "Term", y = "Beta",
         title = "Top 10 High Probability Words by Topic",
         subtitle = "From LDA on Articles Dataset") +
    coord_flip() +
    scale_x_reordered() +
    scale_fill_viridis(discrete = TRUE) +
    eujingc_315_theme
```



***

##  Problem 5


We will be using data from nflscapR which has data from many aspects of football games from the NFL all the way back to 2009.

We have play-by-play data, which contains timeseries data (events) at a game level, recording variables such as:
* Which team is currently in defense
* Distance to opponent endzone
* Player who just intercepted the ball
* Was a rush attempted
* Penalty type
* Amount of yards gained on the play
* and many more categorical and continuous quantities that describe events in a game

We can use such data to answer questions such as:
1) Can we have low-dimension projections of team strategies and see if there are any clusters?
2) Which players are the most valuable in a team?


***
***




***
***
***
***
***
***


